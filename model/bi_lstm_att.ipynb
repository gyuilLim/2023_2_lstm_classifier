{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import nltk.data\n",
    "from annoy import AnnoyIndex\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    " \n",
    "from utils.tokenizer import Tokenize\n",
    "from utils.vectorizer import Vectorize\n",
    "from utils.processing import Preprocessing, check_profanity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Namespace 정의`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    raw_dataset_csv=\"../train.csv\",\n",
    "    preprocessed_csv=\"../preprocessed_train.csv\",\n",
    "    train_proportion=0.8,\n",
    "    val_proportion=0.2,\n",
    "    # 날짜와 경로 정보\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"../model_storage/toxicity_with_splits\",\n",
    "    # 모델 하이퍼파라미터\n",
    "    embedding_size=400,\n",
    "    max_seq_length = 1000,\n",
    "    rnn_hidden_size=256,\n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337,\n",
    "    num_epochs=5,#5,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    early_stopping_criteria=5,\n",
    "    # 실행 옵션\n",
    "    cuda='cuda',\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    vocab_max_length = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Preprocessing(args.raw_dataset_csv)\n",
    "toxicity_df = preprocessing.preprocess_comment()\n",
    "toxicity_df.to_csv(args.preprocessed_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Tokenizer(tokenize + vocabulary)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5871"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = toxicity_df['comment'].values\n",
    "tokenize = Tokenize()\n",
    "tokenized_sents, vocab = tokenize.doc_tokenize(comments, train=True)\n",
    "vocab_len = len(vocab)\n",
    "vocab_len\n",
    "# 22s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Vectorize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1405"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(len(item) for item in tokenized_sents)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = Vectorize(tokenized_sents, vocab, max_length)\n",
    "out_vector, length = vectorize.vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79785, 1405)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_vector = np.array(out_vector)\n",
    "out_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, out_vector, max_length, toxicity_df):\n",
    "        self.out_vector = out_vector\n",
    "        self.target = toxicity_df['toxicity']\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.x_data = self.out_vector\n",
    "        self.y_target = self.target\n",
    "\n",
    "        self.len = len(self.y_target)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_data = self.x_data[index]\n",
    "        y_target = self.y_target[index]\n",
    "\n",
    "        x_data = torch.tensor(x_data)\n",
    "        y_target = torch.tensor(y_target)\n",
    "\n",
    "        ## 여기 수정해야함 </s>(idx:2) 이거 없는 코멘트는 맨 끝 인덱스로 - 수정함\n",
    "        length = torch.where(x_data == 2)[0]\n",
    "        if len(length) == 0:\n",
    "            length = args.max_seq_length#torch.tensor([0])\n",
    "        else:\n",
    "            length = length.item()\n",
    "\n",
    "        return {'x_data': x_data.to(args.cuda),\n",
    "                'y_target': y_target.to(args.cuda),\n",
    "                'x_length': length}\n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, out_vector, max_length, toxicity_df, is_test=False):\n",
    "        self.out_vector = out_vector\n",
    "        self.max_length = max_length\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        if not is_test:\n",
    "            self.target = toxicity_df['toxicity']\n",
    "            self.y_target = torch.tensor(self.target)\n",
    "        else:\n",
    "            self.y_target = None\n",
    "\n",
    "        self.x_data = torch.tensor(self.out_vector)\n",
    "        self.len = len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_data = self.x_data[index]\n",
    "\n",
    "        x_data = torch.tensor(x_data)\n",
    "\n",
    "        length = torch.where(x_data == 2)[0]\n",
    "        if len(length) == 0:\n",
    "            length = self.max_length\n",
    "        else:\n",
    "            length = length.item()\n",
    "\n",
    "        if self.is_test:\n",
    "            return {'x_data': x_data.to(args.cuda),\n",
    "                    'x_length': length}\n",
    "        else:\n",
    "            y_target = self.y_target[index]\n",
    "            y_target = torch.tensor(y_target)\n",
    "            return {'x_data': x_data.to(args.cuda),\n",
    "                    'y_target': y_target.to(args.cuda),\n",
    "                    'x_length': length}\n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, no_layers, vocab_size, hidden_dim, embedding_dim, output_dim, bidirectional, drop_prob=0.5):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # 임베딩 및 LSTM 레이어\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=no_layers, batch_first=True, bidirectional=self.bidirectional)\n",
    "\n",
    "        # 드롭아웃 레이어\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # 완전 연결 및 시그모이드 레이어\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    \n",
    "    def attention_net(self, lstm_output, final_state): # lstm_output - (batch size, maxlen, hidden*2), final_state - (1, batch size, hidden*2)\n",
    "        hidden = final_state.squeeze(0) # (batch size, hidden*2)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2) # (batch size, maxlen)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2) # (batch size, hidden*2)\n",
    "\n",
    "        return new_hidden_state\n",
    "    \n",
    "    def forward(self, x_in, hidden):\n",
    "        batch_size = x_in.size(0)\n",
    "\n",
    "        # 임베딩 및 LSTM 출력\n",
    "        embeds = self.embedding(x_in)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        attn_output = self.attention_net(lstm_out, lstm_out.transpose(0, 1)[-1])\n",
    "\n",
    "        # 드롭아웃과 완전 연결 레이어\n",
    "        attn_output = attn_output.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout(attn_output)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # 시그모이드 활성화 함수\n",
    "        sig_out = self.sig(out)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]  # 배치의 마지막 레이블 가져오기\n",
    "\n",
    "        # 시그모이드 출력과 히든 상태 반환\n",
    "        return sig_out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # 히든 상태 초기화\n",
    "        dim = self.no_layers\n",
    "        if self.bidirectional :\n",
    "            dim = self.no_layers * 2\n",
    "            \n",
    "        h0 = torch.zeros((dim, batch_size, self.hidden_dim)).to(args.cuda)\n",
    "        c0 = torch.zeros((dim, batch_size, self.hidden_dim)).to(args.cuda)\n",
    "        return h0, c0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Prepare`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "\n",
    "# 재현성을 위해 시드 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# 디렉토리 처리\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79785"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('./data/toxicity_with_splits.csv')\n",
    "df = toxicity_df\n",
    "\n",
    "df_dataset = dataset(out_vector, max_length, df)\n",
    "\n",
    "dataset_size = len(df_dataset)\n",
    "train_size = int(dataset_size * args.train_proportion)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "# train_dataset, validation_dataset = random_split(df_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(dataset=df_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False)\n",
    "# val_dataloader = DataLoader(dataset=validation_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "len(df_dataset)#, len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(5871, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# classifier = commentClassifier(embedding_size=args.embedding_size, \n",
    "#                                num_embeddings=len(vocab),\n",
    "#                                num_classes=1,\n",
    "#                                rnn_hidden_size=args.rnn_hidden_size,)\n",
    "\n",
    "no_layers = 2\n",
    "classifier = SentimentRNN(no_layers, vocab_len, args.rnn_hidden_size, args.embedding_size, bidirectional=True, output_dim=1).to(args.cuda)\n",
    "classifier = classifier.to('cuda')\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=0)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "#                                            mode='min', factor=0.5,\n",
    "#                                            patience=1)\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1247 [00:00<?, ?it/s]/tmp/ipykernel_5145/4200132797.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_data = torch.tensor(x_data)\n",
      "/tmp/ipykernel_5145/4200132797.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_target = torch.tensor(y_target)\n",
      "  3%|▎         | 42/1247 [00:09<04:29,  4.48it/s, batch_loss=0.441]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m y_pred, _ \u001b[39m=\u001b[39m classifier(x_in\u001b[39m=\u001b[39mbatch_dict[\u001b[39m'\u001b[39m\u001b[39mx_data\u001b[39m\u001b[39m'\u001b[39m], hidden\u001b[39m=\u001b[39mh)\n\u001b[1;32m     27\u001b[0m loss \u001b[39m=\u001b[39m loss_func(y_pred\u001b[39m.\u001b[39msqueeze(), batch_dict[\u001b[39m'\u001b[39m\u001b[39my_target\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m---> 28\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     29\u001b[0m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(classifier\u001b[39m.\u001b[39mparameters(), clip)\n\u001b[1;32m     30\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "epoch_train_loss = []\n",
    "epoch_val_loss = []\n",
    "epoch_f1_score = []\n",
    "epoch_roc_auc_score = []\n",
    "\n",
    "for epoch_index in range(args.num_epochs):\n",
    "\n",
    "    # ----------------------------------------------train\n",
    "    train_loss = []\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    train_target_labels = []\n",
    "    \n",
    "    clip = 5\n",
    "    classifier.train()\n",
    "\n",
    "    batch_index = 0\n",
    "    train_loop = tqdm(train_dataloader, leave=True)\n",
    "    for batch_dict in train_loop:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        h = classifier.init_hidden(batch_dict['x_data'].shape[0])\n",
    "        y_pred, _ = classifier(x_in=batch_dict['x_data'], hidden=h)\n",
    "        loss = loss_func(y_pred.squeeze(), batch_dict['y_target'].float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(classifier.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        # acc_t = accuracy_score(y_pred, batch_dict['y_target'])\n",
    "        # acc_t = accuracy_score(y_pred.detach().cpu().numpy(), batch_dict['y_target'].detach().cpu().numpy())\n",
    "\n",
    "        y_label = torch.round(y_pred).squeeze()\n",
    "        y_target = batch_dict['y_target']\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        train_preds.extend(y_pred.detach().cpu().numpy())\n",
    "        train_labels.extend(y_label.detach().cpu().numpy())\n",
    "        train_target_labels.extend(y_target.detach().cpu().numpy())\n",
    "\n",
    "        batch_index += 1\n",
    "        train_loop.set_postfix(batch_loss=loss.item())\n",
    "        \n",
    "    # -- train scoring --\n",
    "    train_f1score = f1_score(train_labels, train_target_labels)\n",
    "    train_roc_auc = roc_auc_score(train_labels, train_target_labels)\n",
    "    \n",
    "    \n",
    "    print(\"train --- f1 score: {:.4f}, roc auc: {:.4f}, loss: {:.4f}\".format(train_f1score, train_roc_auc, np.mean(train_loss)))\n",
    "        \n",
    "    # ----------------------------------------------valid\n",
    "    # val_loss = []\n",
    "    # val_preds = []\n",
    "    # val_labels = []\n",
    "    # val_target_labels = []\n",
    "    \n",
    "    # classifier.eval()\n",
    "\n",
    "    # batch_index = 0\n",
    "    # val_loop = tqdm(val_dataloader, leave=True)\n",
    "    # for batch_dict in val_loop:\n",
    "    #     optimizer.zero_grad()\n",
    "        \n",
    "    #     h = classifier.init_hidden(batch_dict['x_data'].shape[0])\n",
    "    #     y_pred, _ = classifier(x_in=batch_dict['x_data'], hidden=h)\n",
    "    #     loss = loss_func(y_pred.squeeze(), batch_dict['y_target'].float())\n",
    "\n",
    "    #     y_label = torch.round(y_pred).squeeze()\n",
    "    #     y_target = batch_dict['y_target']\n",
    "        \n",
    "    #     val_loss.append(loss.item())\n",
    "    #     val_preds.extend(y_pred.detach().cpu().numpy())\n",
    "    #     val_labels.extend(y_label.detach().cpu().numpy())\n",
    "    #     val_target_labels.extend(y_target.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    #     val_loop.set_postfix(batch_loss=loss.item())\n",
    "    \n",
    "    # # -- valid scoring --\n",
    "    # val_f1score = f1_score(val_labels, val_target_labels)\n",
    "    # val_roc_auc = roc_auc_score(val_labels, val_target_labels)\n",
    "    \n",
    "    # print(\"Epoch : \", epoch_index)\n",
    "    # print(\"train --- f1 score: {:.4f}, roc auc: {:.4f}, loss: {:.4f}\".format(train_f1score, train_roc_auc, np.mean(train_loss)))\n",
    "    # print(\"valid --- f1 score: {:.4f}, roc auc: {:.4f}, loss: {:.4f}\".format(val_f1score, val_roc_auc, np.mean(val_loss)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # epoch_train_loss.append(np.mean(train_loss))\n",
    "    # epoch_val_loss.append(np.mean(val_loss))\n",
    "    \n",
    "    # epoch_f1_score.append(val_f1score)\n",
    "    # epoch_roc_auc_score.append(val_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Test inference`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_path = '../2-Toxicity/validation.csv'\n",
    "test_path = '../2-Toxicity/test_for_inference.csv'\n",
    "\n",
    "# preprocessing\n",
    "preprocessing = Preprocessing(test_path)\n",
    "test_toxicity_df = preprocessing.preprocess_comment()\n",
    "# toxicity_df.to_csv('../2-Toxicity/preprocessed_test.csv', index=False)\n",
    "\n",
    "# tokenize\n",
    "comments = test_toxicity_df['comment'].values\n",
    "tokenize = Tokenize()\n",
    "tokenized_sents, _ = tokenize.doc_tokenize(comments, train=True)\n",
    "# vocab_len = len(_)\n",
    "# vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31915, 1343)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize\n",
    "max_length = max(len(item) for item in tokenized_sents)\n",
    "vectorize = Vectorize(tokenized_sents, vocab, max_length)\n",
    "out_vector, length = vectorize.vectorizer()\n",
    "out_vector = np.array(out_vector)\n",
    "out_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset(out_vector, max_length, test_toxicity_df, is_test=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/499 [00:00<?, ?it/s]/tmp/ipykernel_66814/4200132797.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_data = torch.tensor(x_data)\n",
      "100%|██████████| 499/499 [00:33<00:00, 14.97it/s]\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "test_labels = []\n",
    "test_target_labels = []\n",
    "\n",
    "classifier.eval()\n",
    "\n",
    "batch_index = 0\n",
    "test_loop = tqdm(test_dataloader, leave=True)\n",
    "for batch_dict in test_loop:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    h = classifier.init_hidden(batch_dict['x_data'].shape[0])\n",
    "    y_pred, _ = classifier(x_in=batch_dict['x_data'], hidden=h)\n",
    "\n",
    "    y_label = torch.round(y_pred).int().squeeze()\n",
    "    test_preds.extend(y_pred.detach().cpu().numpy())\n",
    "    test_labels.extend(y_label.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    test_loop.set_postfix()\n",
    "\n",
    "# -- valid scoring --\n",
    "# val_f1score = f1_score(val_labels, val_target_labels)\n",
    "# val_roc_auc = roc_auc_score(val_labels, val_target_labels)\n",
    "\n",
    "# print(\"Epoch : \", epoch_index)\n",
    "# print(\"valid --- f1 score: {:.4f}, roc auc: {:.4f}, loss: {:.4f}\".format(val_f1score, val_roc_auc, np.mean(val_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_preds = test_preds.copy()\n",
    "temp_labels = test_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 2 , p = 0.3\n",
    "\n",
    "p = 0.3\n",
    "\n",
    "for idx, comment in enumerate(comments) :\n",
    "    if check_profanity(comment) :\n",
    "        if (temp_preds[idx] + p) <=  1 :\n",
    "            temp_preds[idx] += p\n",
    "            \n",
    "answer = np.round(temp_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submit = pd.read_csv('../2-Toxicity/sample_submission.csv')\n",
    "sample_submit['probability'] = temp_preds\n",
    "sample_submit['pred'] = temp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submit.to_csv('../2-Toxicity/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
